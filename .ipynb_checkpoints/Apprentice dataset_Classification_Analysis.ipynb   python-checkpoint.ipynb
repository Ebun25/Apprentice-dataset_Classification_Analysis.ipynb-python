{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d35c47b",
   "metadata": {},
   "source": [
    "\n",
    "INTRODUCTION\n",
    "The Cross_Sell_Success dataset is a valuable resource for the business  to improve their performance by accurately predicting the outcomes of cross-selling promotions on some factors that could either be a success or failure of the promotion. The Cross_Sell_Success dataset contains 1,946 entries and 17 columns, with no missing values. Our primary goal is to use this dataset to train a machine learning model that can accurately predict future outcomes of success of the cross-sell promotion. To accomplish this, we will focus on identifying the most effective model type and optimizing it for the best possible AUC score. By doing so, we aim to create a model that can reliably and accurately predict outcomes for new datasets or models. This will provide valuable insights that can be used to make informed business decisions and improve overall performance.\n",
    "\n",
    "DATA ANALYSIS\n",
    "The data analysis process began with the importation of all necessary libraries. The dataset was then described in detail, providing insight into its contents. Feature engineering was done to improve the performance and accuracy of machine learning models. The email column was split into three rows and renamed using the iteration and rename functions. One hot encoding was used on the 'mail' variable in the 'sell_success' DataFrame. This technique was employed to convert categorical data into numerical data, making it easier for machine learning models to process and analyze.\n",
    "\n",
    "A correlation analysis was conducted, revealing low or non-existent correlations between the X-variables and the Y-variable (Cross_Sell_Success). The variable with the highest positive correlation was n_email_3, with a correlation value of 0.19. The variable with the highest negative correlation was n_email_2, with a correlation value of -0.28.\n",
    "\n",
    "The code was then split into two separate data frames - one containing the explanatory variables (features) and the other containing the response variable (target). The X-data included the names of the variables deemed the most relevant predictors of cross-sell success, based on previous analysis. This step was crucial in preparing the data for the predictive model, as it involved selecting the most relevant variables and separating the response variable from the explanatory variables. This separation is vital because the explanatory variables are used to train the model, while the response variable is used to evaluate the model's performance.\n",
    "\n",
    "In the development of a machine learning model, it is essential to split the data into training and testing sets to ensure that the model is trained on a subset of the data and tested on an independent subset. This is a crucial step in the model development process as it helps to prevent overfitting and allows for the evaluation of the model's performance on new, unseen data.\n",
    "\n",
    "To achieve this, the Scikit-learn library provides a useful function called train_test_split, which splits the data into training and testing sets. In this particular case, the function was used to split the data into 90% for training and 10% for testing. Additionally, the function was passed the sell_success_data and sell_success_target parameters to represent the independent and dependent variables, respectively. The random_state parameter was set to 219 to ensure that the split is always the same, and the stratify parameter was set to sell_success_target to ensure that the balance of classes in the target variable is preserved in both the training and testing sets.\n",
    "\n",
    "After the data was prepared, logistic regression was used to predict the likelihood of a customer making a cross-sell purchase. The model used several customer attributes, including the number of emails sent, cancellations after noon, mobile logins, average time per site visit, unique meals purchased, PC logins, number of emails opened, and revenue, to analyze the relationship between a binary dependent variable and one or more independent variables.\n",
    "\n",
    "The logistic regression analysis revealed that n_email_3, CANCELLATIONS_AFTER_NOON, and n_email_2 had a minimum p-value of 0.0000 and were statistically significant to the cross-sell success. However, revenue and UNIQUE_MEALS_PURCH were not significant to the cross-sell success, indicating that they may have been some of the reasons why the cross-sell success failed in some areas.\n",
    "\n",
    "The use of train_test_split function and logistic regression analysis were essential steps in the development of the machine learning model. The data was split into training and testing sets, ensuring that the testing set was representative of the data set as a whole, while logistic regression analysis helped to identify which customer attributes were significant to the cross-sell success. From the findings it was seen that n_email_3, CANCELLATIONS_AFTER_NOON, MOBILE_LOGINS, Avg_time_per_visit, pc_logins and n_email_2 were the only significant variables.These findings can be used to optimize the cross-sell strategy and improve the success rate in the future.The logistic regression model to predict the likelihood of a customer purchasing a meal package offered by cross_sell_success. Features from the dataset were selected in which the model will use to make its predictions. \"sell_success_data\" stores the features and \"sell_success_target\" stores the target variable that we want to predict.\n",
    "The \"train_test_split\" function is used to split the dataset into two parts, one part for training the model and the other for testing it. The \"stratify\" parameter is used to ensure that the training and test datasets have similar proportions of the target variable.\n",
    "Then, a logistic regression model is instantiated with certain hyperparameters (solver which was set to 'newton-cgâ€™, C was to 1, and random_state as 219 which are optimized for the given problem. The model is then fit to the training data, and then used to predict the test data.\n",
    "The model's performance is then evaluated using the accuracy metric, which measures the proportion of correct predictions. The accuracy scores of both the training and test datasets are 72%  and 71% respectively the gap LogReg Train-Test gap was 0.0136. \n",
    "The confusion matrix is a useful tool for evaluating the performance of a classification model by providing information on the accuracy and precision of its predictions. In this case, the true positive value was 119, the false positive was 44, the false negative was 13, and the true negative was 19.\n",
    "\n",
    "To further evaluate the performance of the model, the AUC (Area Under the Curve) score for a logistic regression model's prediction on a test dataset was calculated. The roc_auc_score function from the sklearn.metrics module was used, which takes in two parameters: y_true, representing the true target values, and y_score, representing the predicted probabilities of the target values. The calculated AUC score was rounded to four decimal places.\n",
    "\n",
    "The AUC score is a measure of how well a model can distinguish between positive and negative classes. A score of 1 indicates perfect classification, while a score of 0.5 indicates random guessing. In this case, the obtained AUC score of 0.6016 suggests that the model has a limited ability to distinguish between the positive and negative classes. The score of 0.6016 was saved for future use in the variable logreg_auc_score.\n",
    "\n",
    "Other models were instantiated to verify which could give a higher AUC score, such as the decision tree classifier, random forest classifier, and gradient boosting classifier. However, all of these models had an AUC score lower than the 0.6016 obtained with the logistic regression model.\n",
    "\n",
    "Interestingly, the random forest model had a full tree training accuracy of 0.8633, full tree testing accuracy of 0.0028, and full tree AUC score of 0.6334. However, the logistic regression model was still preferred due to its higher testing accuracy.\n",
    "\n",
    "In summary, the confusion matrix and AUC score were used to evaluate the performance of a classification model. The logistic regression model was found to have limited ability to distinguish between positive and negative classes, with an AUC score of 0.6016. Although the random forest model had a higher AUC score, the logistic regression model was still preferred due to its higher testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split \n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "\n",
    "# loading data\n",
    "sell_success = pd.read_excel(io ='./__storage/Cross_Sell_Success_Dataset_2023.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "# displaying the head of the dataset\n",
    "sell_success.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50abbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_success.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b44a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_success.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing the dataset\n",
    "sell_success.describe(include = 'number').round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column called 'mail' in the sell_success DataFrame and setting all the values to 0\n",
    "sell_success['mail'] = 0\n",
    "\n",
    "# Iterate over each row in the sell_success DataFrame\n",
    "for index, row in sell_success.iterrows():\n",
    "# Extract the domain name from the email address using split() and set it to email_domain\n",
    "    email_domain = row['EMAIL'].split('@')[-1]\n",
    "# If the email domain is one of these values, set the 'mail' value for that row to 1\n",
    "    if email_domain in ['gmail.com', 'yahoo.com', 'protonmail.com']:\n",
    "        sell_success.loc[index, 'mail'] = 1\n",
    "# If the email domain is one of these values, set the 'mail' value for that row to 2\n",
    "    elif email_domain in ['me.com', 'aol.com', 'hotmail.com', 'live.com', 'msn.com', 'passport.com']:\n",
    "        sell_success.loc[index, 'mail'] = 2\n",
    "# If the email domain is not one of the above values, set the 'mail' value for that row to 3\n",
    "    else:\n",
    "        sell_success.loc[index, 'mail'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02649367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the 'mail' column in the sell_success DataFrame to integer type\n",
    "sell_success['mail'] = sell_success['mail'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding variables\n",
    "one_hot_email       = pd.get_dummies(sell_success['mail'])\n",
    "\n",
    "# joining codings together\n",
    "sell_success = sell_success.join(other = [one_hot_email])\n",
    "\n",
    "\n",
    "\n",
    "# checking results\n",
    "sell_success.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb75d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reeenaming columns\n",
    "sell_success = sell_success.rename(columns={1: 'n_email_1',2 :'n_email_2', 3:'n_email_3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dabb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting information about Data set\n",
    "sell_success.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bc4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the correlation coefficients\n",
    "sell_corr = sell_success.corr(method = \"pearson\").round(decimals = 2)\n",
    "\n",
    "sell_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "##listing all variables\n",
    "X_data = { 'REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "                   'AVG_TIME_PER_SITE_VISIT', 'CANCELLATIONS_AFTER_NOON', 'PC_LOGINS', 'MOBILE_LOGINS', 'WEEKLY_PLAN', 'LATE_DELIVERIES ',\n",
    "                   'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 'AVG_MEAN_RATING', 'TOTAL_PHOTOS_VIEWED',\n",
    "                    'n_email_2', 'n_email_3'}\n",
    "\n",
    "# declaring explanatory variables\n",
    "sell_success_data =sell_success[X_data]\n",
    "\n",
    "# declaring response variable\n",
    "sell_success_target = sell_success.loc[:, \"CROSS_SELL_SUCCESS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "          sell_success_data,\n",
    "          sell_success_target,\n",
    "            test_size    = 0.10,\n",
    "            random_state = 219,\n",
    "            stratify     = sell_success_target) # preserving balance\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "sell_success_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344bc5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logistic_small = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~ n_email_3+CANCELLATIONS_AFTER_NOON+MOBILE_LOGINS+\n",
    "                                                             AVG_TIME_PER_SITE_VISIT+UNIQUE_MEALS_PURCH+PC_LOGINS+n_email_2+REVENUE\"\"\",\n",
    "                           data    = sell_success_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "results_logistic = logistic_small.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_logistic.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas            as pd                       \n",
    "import matplotlib.pyplot as plt                      \n",
    "import seaborn           as sns                      \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "import statsmodels.formula.api as smf               \n",
    "from sklearn.metrics import confusion_matrix         \n",
    "from sklearn.metrics import roc_auc_score            \n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn.neighbors import KNeighborsRegressor    \n",
    "from sklearn.preprocessing import StandardScaler     \n",
    "from sklearn.tree import DecisionTreeClassifier      \n",
    "from sklearn.tree import plot_tree                   \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary to store success models\n",
    "success_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE', 'PRODUCT_CATEGORIES_VIEWED',\n",
    "                   'AVG_TIME_PER_SITE_VISIT', 'CANCELLATIONS_AFTER_NOON', 'PC_LOGINS', 'MOBILE_LOGINS', 'WEEKLY_PLAN', 'LATE_DELIVERIES ',\n",
    "                   'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 'AVG_MEAN_RATING', 'TOTAL_PHOTOS_VIEWED',\n",
    "                    'n_email_2', 'n_email_3'],\n",
    " \n",
    "\n",
    " # significant variables only (set 1)\n",
    " 'logit_sig'    : ['n_email_3','CANCELLATIONS_AFTER_NOON','MOBILE_LOGINS',\n",
    "                  'AVG_TIME_PER_SITE_VISIT','UNIQUE_MEALS_PURCH','PC_LOGINS',\n",
    "                   'n_email_2','REVENUE'],\n",
    "    \n",
    "    \n",
    " # significant variables only (set 2)\n",
    " 'logit_sig_2'  : ['n_email_3','CANCELLATIONS_AFTER_NOON','MOBILE_LOGINS',\n",
    "                  'AVG_TIME_PER_SITE_VISIT','PC_LOGINS',\n",
    "                   'n_email_2','REVENUE']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing success n@3u7variable sets\n",
    "print(f\"\"\"\n",
    "/--------------------------\\\\\n",
    "|Explanatory Variable Sets |\n",
    "\\\\--------------------------/\n",
    "\n",
    "Full Model:\n",
    "-----------\n",
    "{success_dict['logit_full']}\n",
    "\n",
    "\n",
    "First Significant p-value Model:\n",
    "--------------------------------\n",
    "{success_dict['logit_sig']}\n",
    "\n",
    "\n",
    "Second Significant p-value Model:\n",
    "---------------------------------\n",
    "{success_dict['logit_sig_2']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2392ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "sell_success_data   =  sell_success.loc[ : , success_dict['logit_sig']]\n",
    "sell_success_target =  sell_success.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            sell_success_data,\n",
    "            sell_success_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.10,\n",
    "            stratify     = sell_success_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'newton-cg',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a confusion matrix\n",
    "print(confusion_matrix(y_true = y_test,\n",
    "                       y_pred = logreg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37713650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22217434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(sell_success[success_dict['logit_sig_2']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = x_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13612132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1608c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = LogisticRegression()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = KNeighborsRegressor()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = RandomForestRegressor()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02997655",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\" The final model adopted has the following attributes: \\n\n",
    "\n",
    "    Model Type:          Logistic Regression ({logreg})\n",
    "    Training Accuracy:   {logreg_train_score}\n",
    "    Testing Accuracy:    {logreg_test_score}\n",
    "    Train-Test Gap:      {logreg_test_gap}\n",
    "    AUC Score:           {logreg_auc_score}\n",
    "    Confusion Matrix:    TN: {logreg_tn}, FP: {logreg_fp}, FN: {logreg_fn}, TP:{logreg_tp}\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5bca18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
